* 「以文字指令操控的語音轉換」這個題目簡單介紹：
	* 傳統的語音轉換 (Voice Conversion) 需要 Reference Speech，並依據此 Reference Speech 的風格進行轉換。
	* 這個題目想做的是**利用「文字」 (Text Instruction) 來取代 Reference Speech**，利用自然語言來指涉語音轉換中的風格變換。
	* 譬如，從簡單的指令 “Speak in the sorrowful tone.” 到更複雜的指令 “Speak if you are telling a bedtime story to the children.”。
* 接下來可能專注於幾個面向：
	* 改善語音的效果與品質。
	* 研究 Unsupervised 的訓練方法。由於目前都需要 Supervised Training，且訓練資料 (可參考老師提供的投影片，我們需要 Triple Data 包含 Source speech, Instruction, Target speech) 難以取得，所以我們可能需要更彈性的作法。
	* Alan: 可以用少量標註去訓練大量資料嗎？ self-supervised learning
	* 現在都是用語音合成的 希望用真實世界的 data
	* CV：有沒有可以text-guiding的部分
		* 文字當作instruction: 去做特定的任務
	* RL： reward function 衡量VC轉換效果
		* 找不同的 style reward models / speaker similarity / semantic information
		* 評分：style caption models
		* diversity 有限
		* 多種不同的 reward models
	* RLHF
	* Matrix Gate
	* PSG speaker similarity
	* [https://github.com/gabrielmittag/NISQA](https://github.com/gabrielmittag/NISQA)
	* 讓reward model分數比較高
	* 可以參考不同領域的如CV
* Towards General-Purpose Text-Instruction-Guided Voice Conversion
	* 與傳統依賴參考語音來定義轉換語音屬性的方法不同，此模型使用文本指令作為風格提示，增加了轉換過程的多樣性和特定性。
	* 這個VC模型基於一種神經編碼語言模型，處理一系列離散代碼，從而產生轉換語音的代碼序列。這個模型能夠根據提供的文本指令修改給定語音的韻律和情感信息。與之前使用不同語音方面的單獨編碼器的方法不同，此模型提供了一種端到端的解決方案。論文通過各種實驗展示了模型理解指令並產生合理結果的令人印象深刻的能力。
	* 為了訓練這個模型，研究人員創建了兩個數據集：信號處理效果數據集和InstructSpeech數據集。這些數據集包含了廣泛的語音樣本、指令和目標語音樣本。模型的評估包括自動和人工評估，重點是韻律相關因素（音高、音量、速度）以及按照指令準確轉換風格的能力。
	* 研究的發現突顯了該模型理解和執行文本指令的強大能力，為未來語音轉換技術的發展指出了一個有前景的方向。作者計劃在未來的工作中進一步提高語音質量並擴展指令類型的多樣性和豐富性。
* High Fidelity Neural Audio Compression [Meta AI FAIR Team]
	* **codec**: 使用神經網絡模型來壓縮和還原音頻數據。
* AudioGen: Textually Guided Audio Generation [Meta AI FAIR Team]
	* 利用文本描述來引導音頻的生成。
* PromptTTS: Controllable Text-to-Speech with Text Descriptions [Microsoft]
	* 根據文本描述來控制語音的風格和內容。
* Audiobox: Unified Audio Generation with Natural Language Prompts [Meta AI FAIR Team]
	* 一種統一的音頻生成模型，能夠根據自然語言提示生成各種音頻形式，包括語音和聲音效果。
* VALL-E: Neural Codec Language Models are Zero-Shot Text-to-Speech Synthesizers [Microsoft]
	* 基於語言模型的文字到語音（TTS）框架，使用音頻編碼模型中的離散代碼作為中間表示。