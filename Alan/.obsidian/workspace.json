{
  "main": {
    "id": "fb40f724c3614a21",
    "type": "split",
    "children": [
      {
        "id": "380cf8b56c9b30e9",
        "type": "tabs",
        "children": [
          {
            "id": "68c6db82825ff399",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "DRL 2018/Lecture 1 Policy Gradient (Review).md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "4700ee57715be013",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "DRL 2018/Lecture 2 Proximal Policy Optimization (PPO).md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "793112bd6d699a01",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "DRL 2018/Lecture 5 Q-learning (Continuous Action).md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "af975caa7a9c1128",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "ML 2021/12 Reinforcement Learning.md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "5922f1691dda85d6",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "DRL 2018/Lecture 6 Actor-Critic.md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "d2849ab90a34df67",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "DRL 2018/Lecture 7 Sparse Reward.md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "4ef2abfe7263f43d",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "Leading Enterprise/W11.md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "d0c08b79d28535d6",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "Web3/Lesson 1 金融市場與加密貨幣概括與個案分析.md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "d96ddcdf975f7ece",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "DRL 2018/Lecture 7 Sparse Reward.md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "1c55af54295bee37",
            "type": "leaf",
            "state": {
              "type": "release-notes",
              "state": {
                "currentVersion": "1.5.12"
              }
            }
          },
          {
            "id": "a95ed7f160c4bc92",
            "type": "leaf",
            "state": {
              "type": "graph",
              "state": {}
            }
          },
          {
            "id": "4d5478a5836f77d3",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "ML 2021/00 Pytorch tutorial.md",
                "mode": "source",
                "source": false
              }
            }
          }
        ],
        "currentTab": 9
      }
    ],
    "direction": "vertical"
  },
  "left": {
    "id": "b81e56c2351e2a6f",
    "type": "split",
    "children": [
      {
        "id": "11e490970338b9d9",
        "type": "tabs",
        "children": [
          {
            "id": "af8b03a407d75688",
            "type": "leaf",
            "state": {
              "type": "file-explorer",
              "state": {
                "sortOrder": "alphabetical"
              }
            }
          },
          {
            "id": "c21aa537cef63246",
            "type": "leaf",
            "state": {
              "type": "search",
              "state": {
                "query": "",
                "matchingCase": false,
                "explainSearch": false,
                "collapseAll": false,
                "extraContext": false,
                "sortOrder": "alphabetical"
              }
            }
          },
          {
            "id": "022e8caa0f3e23bf",
            "type": "leaf",
            "state": {
              "type": "starred",
              "state": {}
            }
          },
          {
            "id": "e19ff738242b70b2",
            "type": "leaf",
            "state": {
              "type": "bookmarks",
              "state": {}
            }
          }
        ]
      }
    ],
    "direction": "horizontal",
    "width": 205.5
  },
  "right": {
    "id": "64a7ec702ff9d1af",
    "type": "split",
    "children": [
      {
        "id": "3a25fcb35860e231",
        "type": "tabs",
        "children": [
          {
            "id": "bb52d6f539971d91",
            "type": "leaf",
            "state": {
              "type": "backlink",
              "state": {
                "file": "DRL 2018/Lecture 7 Sparse Reward.md",
                "collapseAll": false,
                "extraContext": false,
                "sortOrder": "alphabetical",
                "showSearch": false,
                "searchQuery": "",
                "backlinkCollapsed": false,
                "unlinkedCollapsed": true
              }
            }
          },
          {
            "id": "86e669294335b2fb",
            "type": "leaf",
            "state": {
              "type": "outgoing-link",
              "state": {
                "file": "DRL 2018/Lecture 7 Sparse Reward.md",
                "linksCollapsed": false,
                "unlinkedCollapsed": true
              }
            }
          },
          {
            "id": "590b2176a924d0fb",
            "type": "leaf",
            "state": {
              "type": "tag",
              "state": {
                "sortOrder": "frequency",
                "useHierarchy": true
              }
            }
          },
          {
            "id": "0cbbb0619b8f8f50",
            "type": "leaf",
            "state": {
              "type": "outline",
              "state": {
                "file": "DRL 2018/Lecture 7 Sparse Reward.md"
              }
            }
          },
          {
            "id": "df671961729311ed",
            "type": "leaf",
            "state": {
              "type": "advanced-tables-toolbar",
              "state": {}
            }
          }
        ],
        "currentTab": 4
      }
    ],
    "direction": "horizontal",
    "width": 300,
    "collapsed": true
  },
  "left-ribbon": {
    "hiddenItems": {
      "switcher:Open quick switcher": false,
      "graph:Open graph view": false,
      "canvas:Create new canvas": false,
      "daily-notes:Open today's daily note": false,
      "templates:Insert template": false,
      "command-palette:Open command palette": false,
      "table-editor-obsidian:Advanced Tables Toolbar": false
    }
  },
  "active": "1c55af54295bee37",
  "lastOpenFiles": [
    "DRL 2018/Lecture 6 Actor-Critic.md",
    "DRL 2018/Lecture 5 Q-learning (Continuous Action).md",
    "DRL 2018/Lecture 4 Q-learning (Advanced Tips).md",
    "DRL 2018/Lecture 3 Q-learning (Basic Idea).md",
    "DRL 2018/Lecture 2 Proximal Policy Optimization (PPO).md",
    "DRL 2018/Lecture 1 Policy Gradient (Review).md",
    "Leading Enterprise/W13.md",
    "Leading Enterprise/W12.md",
    "Others/2024-01-03.md",
    "ML 2021/00 Pytorch tutorial.md",
    "DLHLP 2020/Deep Learning for Human Language Processing (Course Overview).md",
    "Web3/Lesson 2 加密貨幣交易所.md",
    "Web3/Lesson 1 金融市場與加密貨幣概括與個案分析.md",
    "Interview/Worldquant.md",
    "Leading Enterprise/W11.md",
    "DRL 2018/Lecture 7 Sparse Reward.md",
    "DRL 2018/Lecture 8 Imitation Learning.md",
    "DRL 2018/attachments/Pasted image 20240504180627.png",
    "DRL 2018/attachments/Pasted image 20240504180454.png",
    "DRL 2018/attachments/Pasted image 20240504164903.png",
    "DRL 2018/attachments/Pasted image 20240504164413.png",
    "DRL 2018/attachments/Pasted image 20240504164051.png",
    "DRL 2018/attachments/Pasted image 20240504162426.png",
    "DRL 2018/attachments/Pasted image 20240504160625.png",
    "DRL 2018/attachments/Pasted image 20240504155957.png",
    "DRL 2018/attachments/Pasted image 20240504155108.png",
    "DRL 2018/attachments/Pasted image 20240503164624.png",
    "Leading Enterprise/W1.md",
    "Leading Enterprise/W10.md",
    "ML 2021/12 Reinforcement Learning.md",
    "Untitled",
    "ML 2021/01 Overview.md",
    "ML 2021/02 Deep Learning - General Guidance.md",
    "ML 2021/05 Transformer.md",
    "ML 2021/04 Self Attention.md",
    "ML 2021/03 CNN.md",
    "ML 2021/02 Deep Learning - 類神經網路優化技巧.md",
    "TOEFL/Writing/attachments",
    "TOEFL/Speaking",
    "TOEFL/Writing",
    "TOEFL/Listening",
    "Web3",
    "DRL 2018/attachments",
    "WorldQuant/attachments",
    "ML 2021/attachments",
    "DRL 2018",
    "Untitled.canvas",
    "ML 2021/Untitled.canvas"
  ]
}